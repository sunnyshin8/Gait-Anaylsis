{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, BatchNormalization\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepaths:  23560    labels:  23560\n"
     ]
    }
   ],
   "source": [
    "# Set your data directories\n",
    "# data_dir = '../Gait Analysis/'\n",
    "img_height = 64\n",
    "img_width = 64\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# Create a label mapping\n",
    "class_labels = {'normal': 0, 'abnormal': 1}\n",
    "\n",
    "# Load images and labels\n",
    "dirlist=[abnormal_dir,healthy_dir ]\n",
    "classes=['Yes', 'No']\n",
    "filepaths=[]\n",
    "labels=[]\n",
    "for i,j in zip(dirlist, classes):\n",
    "    filelist=os.listdir(i)\n",
    "    for f in filelist:\n",
    "        filepath=os.path.join (i,f)\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(j)\n",
    "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "images = images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'normal' images: 0\n",
      "Number of 'abnormal' images: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6656\\2972066377.py:2: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  num_normal_images = np.sum(labels == class_labels['normal'])\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_6656\\2972066377.py:3: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  num_abnormal_images = np.sum(labels == class_labels['abnormal'])\n"
     ]
    }
   ],
   "source": [
    "# Count the number of images for each label\n",
    "num_normal_images = np.sum(labels == class_labels['normal'])\n",
    "num_abnormal_images = np.sum(labels == class_labels['abnormal'])\n",
    "\n",
    "print(\"Number of 'normal' images:\", num_normal_images)\n",
    "print(\"Number of 'abnormal' images:\", num_abnormal_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Fseries \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(X, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfilepaths\u001b[39m\u001b[39m\"\u001b[39m,dtype\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m)\n\u001b[0;32m      2\u001b[0m Lseries \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(y, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m,dtype\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m)\n\u001b[0;32m      3\u001b[0m tumor_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([Fseries,Lseries], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "Fseries = pd.Series(X, name=\"filepaths\",dtype=str)\n",
    "Lseries = pd.Series(y, name=\"labels\",dtype=str)\n",
    "tumor_data = pd.concat([Fseries,Lseries], axis=1)\n",
    "tumor_df = pd.DataFrame(tumor_data)\n",
    "\n",
    "print(tumor_df.head())\n",
    "print(\"---------------------\")\n",
    "print(tumor_df[\"labels\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tumor_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#splitting data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m train_images, test_images \u001b[39m=\u001b[39m train_test_split(tumor_df, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m train_set, val_set \u001b[39m=\u001b[39m train_test_split(tumor_df, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tumor_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, test_images = train_test_split(tumor_df, test_size=0.3, random_state=42)\n",
    "train_set, val_set = train_test_split(tumor_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape your image data to have one timestep per row\n",
    "x_train = x_train.reshape(x_train.shape[0], img_height, img_width * 3)\n",
    "x_val = x_val.reshape(x_val.shape[0], img_height, img_width * 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_height, img_width * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "589/589 [==============================] - 530s 893ms/step - loss: 0.7441 - accuracy: 0.5172 - val_loss: 0.6912 - val_accuracy: 0.5446\n",
      "Epoch 2/10\n",
      "589/589 [==============================] - 422s 716ms/step - loss: 0.6944 - accuracy: 0.5348 - val_loss: 0.6824 - val_accuracy: 0.5739\n",
      "Epoch 3/10\n",
      "589/589 [==============================] - 417s 707ms/step - loss: 0.6855 - accuracy: 0.5647 - val_loss: 0.6874 - val_accuracy: 0.5216\n",
      "Epoch 4/10\n",
      "589/589 [==============================] - 630s 1s/step - loss: 0.6886 - accuracy: 0.5345 - val_loss: 0.6932 - val_accuracy: 0.4053\n",
      "Epoch 5/10\n",
      "589/589 [==============================] - 497s 843ms/step - loss: 0.6910 - accuracy: 0.5413 - val_loss: 0.6814 - val_accuracy: 0.6057\n",
      "Epoch 6/10\n",
      "589/589 [==============================] - 670s 1s/step - loss: 0.6685 - accuracy: 0.6040 - val_loss: 0.6727 - val_accuracy: 0.6184\n",
      "Epoch 7/10\n",
      "589/589 [==============================] - 497s 843ms/step - loss: 0.5299 - accuracy: 0.7512 - val_loss: 0.4542 - val_accuracy: 0.7984\n",
      "Epoch 8/10\n",
      "589/589 [==============================] - 617s 1s/step - loss: 0.3698 - accuracy: 0.8583 - val_loss: 0.3793 - val_accuracy: 0.8098\n",
      "Epoch 9/10\n",
      "589/589 [==============================] - 610s 1s/step - loss: 0.3185 - accuracy: 0.8775 - val_loss: 0.3353 - val_accuracy: 0.8943\n",
      "Epoch 10/10\n",
      "589/589 [==============================] - 496s 842ms/step - loss: 0.2866 - accuracy: 0.8881 - val_loss: 0.2320 - val_accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(img_height, img_width * 3), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Third LSTM layer\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with class weights\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data and early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(x_val, y_val),  \n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weight_dict  # Pass class weights here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 16s 219ms/step - loss: 0.2196 - accuracy: 0.9189\n",
      "Test Loss: 0.21961523592472076\n",
      "Test Accuracy: 0.9189304113388062\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
